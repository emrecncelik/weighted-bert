{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from weighted_bert.models import WeightedAverage, WeightedRemoval\n",
    "from weighted_bert.data import InputExample\n",
    "\n",
    "logging.basicConfig(level=\"DEBUG\")\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a HuggingFace model for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighting_checkpoint = \"savasy/bert-base-turkish-ner-cased\"\n",
    "embedding_checkpoint = \"emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\"\n",
    "\n",
    "documents = [\n",
    "[\n",
    "    \"Tesla'nın otomobilleri insan hayatlarını riske atıyor olabilir.\",\n",
    "    \"Türkiye ve Kore arasında gerçekleşen voleybol müsabakasını Türkiye Milli Takımı kazandı.\",\n",
    "    \"Bu bir metin.\",\n",
    "],\n",
    "[\n",
    "    \"Mustafa Kemal Atatürk 19 Mayıs 1919'da Samsun'a ayak bastı.\",\n",
    "    \"Bu bir metin.\",\n",
    "],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "INFO:weighted_bert.models:================ Loading weighting model ================\n",
      "INFO:weighted_bert.models:\tsavasy/bert-base-turkish-ner-cased\n",
      "/home/emrecan/python-env/base/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:128: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n",
      "INFO:weighted_bert.models:================ Loading weighting model ================\n",
      "INFO:weighted_bert.models:\tsavasy/bert-base-turkish-ner-cased\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "embedding_model = SentenceTransformer(embedding_checkpoint)\n",
    "weighter_a = WeightedAverage(weighting_checkpoint)\n",
    "weighter_r = WeightedRemoval(weighting_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.33it/s]\n",
      "/home/emrecan/python-env/base/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [1, 3, 0]\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [2, 0]\n",
      "INFO:weighted_bert.models:================ Detecting entities ================\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [1, 3, 0]\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [2, 0]\n",
      "INFO:weighted_bert.models:================ Calculating initial document embeddings ================\n",
      "INFO:weighted_bert.models: ================ Calculating first singular vector  ================\n",
      "INFO:weighted_bert.models:================ Calculating initial document embeddings ================\n",
      "INFO:weighted_bert.models:================ Calculating corrected embeddings ================\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings\n",
    "input_examples = [InputExample(doc, embedding_model.encode(doc)) for doc in documents]\n",
    "embeds_a = weighter_a.fit_transform(input_examples)\n",
    "embeds_r = weighter_r.fit_transform(input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 768), (2, 768))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighter_a.embeddings_.shape, weighter_r.embeddings_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a rule based entity detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Any\n",
    "\n",
    "def detect(sentence: str):\n",
    "    sentence_entites = [] \n",
    "    entity_list = ['tesla', \"atatürk\", \"türkiye\"]\n",
    "\n",
    "    for ent in entity_list:\n",
    "        matches = re.finditer(ent, sentence.lower())\n",
    "        indexes = [(match.start(), match.end()) for match in matches]\n",
    "        if indexes:\n",
    "            for start, end in indexes:\n",
    "                sentence_entites.append({\"text\": ent, \"start\": start, \"end\": end})\n",
    "    \n",
    "    return sentence_entites\n",
    "\n",
    "def entity_detector(document: List[str]) -> List[List[Any]]:\n",
    "    return [detect(sentence) for sentence in document]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: emrecan/bert-base-turkish-cased-mean-nli-stsb-tr\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cuda\n",
      "INFO:weighted_bert.models:================ Loading weighting model ================\n",
      "INFO:weighted_bert.models:\tsavasy/bert-base-turkish-ner-cased\n",
      "INFO:weighted_bert.models:================ Loading weighting model ================\n",
      "INFO:weighted_bert.models:\tsavasy/bert-base-turkish-ner-cased\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "embedding_model = SentenceTransformer(embedding_checkpoint)\n",
    "weighter_a = WeightedAverage(weighting_checkpoint)\n",
    "weighter_r = WeightedRemoval(weighting_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 67.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 87.90it/s]\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [1, 3, 0]\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [2, 0]\n",
      "INFO:weighted_bert.models:================ Detecting entities ================\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [1, 3, 0]\n",
      "DEBUG:weighted_bert.models:Entity count list for doc: [2, 0]\n",
      "INFO:weighted_bert.models:================ Calculating initial document embeddings ================\n",
      "INFO:weighted_bert.models: ================ Calculating first singular vector  ================\n",
      "INFO:weighted_bert.models:================ Calculating initial document embeddings ================\n",
      "INFO:weighted_bert.models:================ Calculating corrected embeddings ================\n"
     ]
    }
   ],
   "source": [
    "# Calculate embeddings\n",
    "input_examples = [InputExample(doc, embedding_model.encode(doc)) for doc in documents]\n",
    "embeds_a = weighter_a.fit_transform(input_examples)\n",
    "embeds_r = weighter_r.fit_transform(input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 768), (2, 768))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds_a.shape, embeds_r.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83bf2d3bd26546d38523b2bf511d3591181790cbdbf44d7b124f51cd603b50a8"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
